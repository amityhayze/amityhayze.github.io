<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Transmission Visualizer</title>
  <style>
    body {
      margin: 0;
      background-color: #000;
      overflow: hidden;
      color: white;
      font-family: monospace;
    }

    #topBar {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      display: flex;
      justify-content: space-between;
      padding: 10px;
      background: rgba(0, 0, 0, 0.5);
      z-index: 10;
    }

    #leftButtons, #rightButtons {
      display: flex;
      gap: 10px;
    }

    button {
      background: rgba(255,255,255,0.1);
      color: white;
      border: 1px solid rgba(255,255,255,0.2);
      padding: 6px 10px;
      cursor: pointer;
      border-radius: 5px;
    }

    button:hover {
      background: rgba(255,255,255,0.2);
    }

    #viz {
      width: 100vw;
      height: 100vh;
      display: block;
    }

    #presetPanel {
      position: fixed;
      top: 50px;
      left: 10px;
      background: rgba(0,0,0,0.8);
      border: 1px solid rgba(255,255,255,0.2);
      padding: 10px;
      border-radius: 8px;
      max-height: 80vh;
      overflow-y: auto;
      z-index: 11;
      transition: transform 0.3s ease;
    }

    #presetPanel.collapsed {
      transform: translateX(-110%);
    }

    #recIndicator {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background: red;
      width: 15px;
      height: 15px;
      border-radius: 50%;
      display: none;
    }

    #recIndicator.show {
      display: block;
      animation: blink 1s infinite;
    }

    @keyframes blink {
      50% { opacity: 0.3; }
    }

    #statusArea {
      position: fixed;
      bottom: 10px;
      left: 10px;
      color: #ccc;
      font-size: 14px;
    }
  </style>
</head>
<body>
  <div id="topBar">
    <div id="leftButtons">
      <button id="backBtn">Back</button>
      <button id="togglePresets">Presets</button>
    </div>
    <div id="rightButtons">
      <input type="file" id="fileInput" accept="audio/*" style="display:none;">
      <button id="uploadBtn">Import</button>
      <button id="snapshotBtn">Snapshot</button>
      <button id="recordBtn">Record</button>
      <button id="fullscreenBtn">Fullscreen</button>
    </div>
  </div>

  <canvas id="viz"></canvas>

  <div id="presetPanel" class="collapsed">
    <p>Preset List Here</p>
  </div>

  <div id="recIndicator"></div>
  <div id="statusArea"></div>

  <script src="https://cdn.jsdelivr.net/npm/@ffmpeg/ffmpeg@0.12.5/dist/ffmpeg.min.js"></script>
  <script>
    const viz = document.getElementById('viz');
    const ctx = viz.getContext('2d');
    const recordBtn = document.getElementById('recordBtn');
    const recIndicator = document.getElementById('recIndicator');
    const statusArea = document.getElementById('statusArea');
    const fullscreenBtn = document.getElementById('fullscreenBtn');
    const uploadBtn = document.getElementById('uploadBtn');
    const fileInput = document.getElementById('fileInput');
    const snapshotBtn = document.getElementById('snapshotBtn');
    const presetToggle = document.getElementById('togglePresets');
    const presetPanel = document.getElementById('presetPanel');

    let audioCtx, sourceNode, analyser, mediaRecorder, recordedChunks = [];
    let recording = false;
    let recordingDest;

    function logStatus(msg) {
      statusArea.textContent = msg;
    }

    function resizeCanvas() {
      viz.width = window.innerWidth;
      viz.height = window.innerHeight;
    }
    window.addEventListener('resize', resizeCanvas);
    resizeCanvas();

    function initAudio(file) {
      if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048;

      if (file) {
        const reader = new FileReader();
        reader.onload = async e => {
          const audioBuffer = await audioCtx.decodeAudioData(e.target.result);
          if (sourceNode) sourceNode.disconnect();
          sourceNode = audioCtx.createBufferSource();
          sourceNode.buffer = audioBuffer;
          sourceNode.connect(analyser);
          analyser.connect(audioCtx.destination);
          sourceNode.start(0);
        };
        reader.readAsArrayBuffer(file);
      }
    }

    function draw() {
      requestAnimationFrame(draw);
      if (!analyser) return;
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      analyser.getByteFrequencyData(dataArray);
      ctx.fillStyle = 'rgba(0,0,0,0.3)';
      ctx.fillRect(0,0,viz.width,viz.height);
      const barWidth = (viz.width / bufferLength) * 2.5;
      let x = 0;
      for(let i = 0; i < bufferLength; i++){
        const barHeight = dataArray[i];
        ctx.fillStyle = `hsl(${i/2}, 100%, 50%)`;
        ctx.fillRect(x, viz.height - barHeight, barWidth, barHeight);
        x += barWidth + 1;
      }
    }
    draw();

    // === UI Buttons ===
    uploadBtn.addEventListener('click', () => fileInput.click());
    fileInput.addEventListener('change', e => initAudio(e.target.files[0]));
    snapshotBtn.addEventListener('click', () => {
      const a = document.createElement('a');
      a.href = viz.toDataURL('image/png');
      a.download = 'snapshot_' + new Date().toISOString().replace(/[:.]/g, '-') + '.png';
      a.click();
    });
    fullscreenBtn.addEventListener('click', () => {
      if (viz.requestFullscreen) viz.requestFullscreen();
      else if (viz.webkitRequestFullscreen) viz.webkitRequestFullscreen();
    });
    presetToggle.addEventListener('click', () => {
      presetPanel.classList.toggle('collapsed');
    });

    // === RECORDING ===
    recordBtn.addEventListener('click', async () => {
      if (!recording) {
        if (!audioCtx) initAudio();
        if (!recordingDest) recordingDest = audioCtx.createMediaStreamDestination();
        if (sourceNode && analyser) {
          sourceNode.disconnect();
          sourceNode.connect(analyser);
          analyser.connect(recordingDest);
          analyser.connect(audioCtx.destination);
        }

        const canvasStream = viz.captureStream(60);
        const audioTracks = recordingDest.stream.getAudioTracks();
        const combined = new MediaStream([...canvasStream.getVideoTracks(), ...audioTracks]);

        mediaRecorder = new MediaRecorder(combined, { mimeType: 'video/webm;codecs=vp9,opus' });
        recordedChunks = [];

        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) recordedChunks.push(e.data);
        };

        mediaRecorder.onstop = async () => {
          logStatus('Processing recording...');
          recIndicator.classList.remove('show');
          const blob = new Blob(recordedChunks, { type: 'video/webm' });
          const arrayBuffer = await blob.arrayBuffer();

          // --- FFmpeg Conversion to MP4 ---
          const { createFFmpeg, fetchFile } = FFmpeg;
          const ffmpeg = createFFmpeg({ log: false });
          await ffmpeg.load();
          await ffmpeg.FS('writeFile', 'input.webm', new Uint8Array(arrayBuffer));
          await ffmpeg.run('-i', 'input.webm', '-c:v', 'libx264', '-c:a', 'aac', '-b:a', '192k', 'output.mp4');
          const mp4Data = ffmpeg.FS('readFile', 'output.mp4');
          const mp4Blob = new Blob([mp4Data.buffer], { type: 'video/mp4' });
          const url = URL.createObjectURL(mp4Blob);

          const a = document.createElement('a');
          a.href = url;
          a.download = 'transmission_recording_' + new Date().toISOString().replace(/[:.]/g, '-') + '.mp4';
          a.click();

          URL.revokeObjectURL(url);
          logStatus('Recording saved as MP4.');
          recording = false;
          recordBtn.textContent = 'Record';
        };

        mediaRecorder.start();
        recording = true;
        recordBtn.textContent = 'Stop Recording';
        recIndicator.classList.add('show');
        logStatus('Recording started...');
      } else {
        mediaRecorder.stop();
      }
    });
  </script>
</body>
</html>
